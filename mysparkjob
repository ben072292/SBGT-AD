#!/bin/bash
#SBATCH --nodes=3
#SBATCH --time=2-00:00:00
#SBATCH --partition long
#SBATCH --job-name=myjob

export JAVA_HOME=/home/hqi6/jdk-17.0.2/bin
export PATH=$JAVA_HOME:$PATH

scontrol show hostname > spark.list

NODE_LIST=`cat spark.list`
#MASTER=`echo $NODE_LIST | cut -d" " -f1`
#SLAVES=`echo $NODE_LIST | cut -d" " -f2-`
MASTER=`head -1 spark.list`
SLAVES=`sed '1d' spark.list`
NODE_NUM=`cat spark.list | wc -l`
CORES_PER_NODE=56
PARALLELISM = $CORES_PER_NODE * $NODE_NUM
SPARK_HOME=/home/hqi6/spark-3.2.1-bin-hadoop3.2

MY_HOST=`hostname -s`
echo $MY_HOST
echo $MASTER
echo $SLAVES

SPARK_CONF_DIR="/spark-3.2.1-bin-hadoop3.2/conf"


echo $MASTER > ${SPARK_CONF_DIR}/master-ip
echo $SLAVES > ${SPARK_CONF_DIR}/workers

sed "s/master/$master-ip/g" my-spark-defaults.conf > spark-defaults.conf
sed -i "$aspark.default.parallelism $PARALLELISM" spark-defaults.conf

sh run.sh $SPARK_HOME